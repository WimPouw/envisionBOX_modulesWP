{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93545df",
   "metadata": {},
   "source": [
    "# Automatic Stimuli Creation for Degrading Visual Information about Articulation or Mouthing\n",
    "<br>\n",
    "<div align=\"center\">Wim Pouw (wim.pouw@donders.ru.nl)</div>\n",
    "\n",
    "<img src=\"Images/envision_banner.png\" alt=\"isolated\" width=\"300\"/>\n",
    "\n",
    "## Info documents\n",
    "\n",
    "This python notebook runs you through the procedure of taking videos as inputs with a single person in the video, and outputting the 1 outputs of the kinematic timeseries, and optionally masking video with facial, hand, and arm kinematics ovelayen.\n",
    "\n",
    "The masked-piper tool is a simple but effective modification of the the Holistic Tracking by Google's Mediapipe so that we can use it as a CPU-based light weigth tool to mask your video data while maintaining background information, and also preserving information about body kinematics. \n",
    "\n",
    "* location Repository:  https://github.com/WimPouw/envisionBOX_modulesWP/tree/main/Mediapipe_Optional_Masking\n",
    "\n",
    "* location Jupyter notebook: https://github.com/WimPouw/envisionBOX_modulesWP/blob/main/MultimodalMerging/Masking_Mediapiping.ipynb\n",
    "\n",
    "Current Github: https://github.com/WimPouw/TowardsMultimodalOpenScience\n",
    "\n",
    "## Additional information backbone of the tool (Mediapipe Holistic Tracking)\n",
    "https://google.github.io/mediapipe/solutions/holistic.html\n",
    "\n",
    "## Citation of mediapipe\n",
    "citation: Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., ... & Grundmann, M. (2019). Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172.\n",
    "\n",
    "## Citation of masked piper\n",
    "* citation: Owoyele, B., Trujillo, J., De Melo, G., & Pouw, W. (2022). Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information. SoftwareX, 20, 101236. \n",
    "* Original Repo: https://github.com/WimPouw/TowardsMultimodalOpenScience\n",
    "\n",
    "## Modification that is the basis of this tool\n",
    "Our modification of the Mediapipe tool is using the body sillhoette to distinguish the background from the body contained in the video, then track the body, and create new video that only keeps the background, masks the body, and overlays the kinematics back onto the mask. We further modify the original code so that timeseries are produced that provide all the kinematic information per frame over time.\n",
    "\n",
    "## Use\n",
    "Make sure to install all the packages in requirements.txt. Then move your videos that you want to mask into the input folder. Then run this code, which will loop through all the videos contained in the input folder; and saves all the results in the output folders.\n",
    "\n",
    "Please use, improve and adapt as you see fit.\n",
    "\n",
    "Team: Babajide Owoyele, James Trujillo, Gerard de Melo, Wim Pouw (wim.pouw@donders.ru.nl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f423ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder is set as the output folder where all the pose time series are stored\n",
      "d:\\Research_projects\\envisionBOX_modulesWP\\MouthRegionMasking\\Output_TimeSeries\n",
      "\n",
      " The following folder is set as the output folder for saving the masked videos \n",
      "d:\\Research_projects\\envisionBOX_modulesWP\\MouthRegionMasking\\Output_Videos\n",
      "\n",
      " The following video(s) will be processed for masking: \n",
      "['DOLFIJN.mp4', 'ETEN.mp4', 'NULL.mp4', 'OCHTEND.mp4', 'OLIFANT.mp4', 'RIETJE.mp4']\n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./Input_Videos/\" #this is your folder with (all) your video(s)\n",
    "vfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))] #loop through the filenames and collect them in a list\n",
    "#time series output folder\n",
    "inputfol = \"./Input_Videos/\"\n",
    "outputf_mask = \"./Output_Videos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outtputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_mask))\n",
    "print(\"\\n The following video(s) will be processed for masking: \")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1815d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06712ae9",
   "metadata": {},
   "source": [
    "## Main procedure Masked-Piper\n",
    "The following chunk of code loops through all the videos you have loaded into the input folder, then assess each frame for body poses, extract kinematic info, masks the body in a new frame that keeps the background, projects the kinematic info on the mask, and stores the kinematic info for that frame into the time series .csv for the hand + body + face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c994ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "DOLFIJN.mp4\n",
      "This is video number 0 of 6 videos in total\n",
      "We will now process video:\n",
      "ETEN.mp4\n",
      "This is video number 1 of 6 videos in total\n",
      "We will now process video:\n",
      "NULL.mp4\n",
      "This is video number 2 of 6 videos in total\n",
      "We will now process video:\n",
      "OCHTEND.mp4\n",
      "This is video number 3 of 6 videos in total\n",
      "We will now process video:\n",
      "OLIFANT.mp4\n",
      "This is video number 4 of 6 videos in total\n",
      "We will now process video:\n",
      "RIETJE.mp4\n",
      "This is video number 5 of 6 videos in total\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# do you want to apply masking?\n",
    "masking = True\n",
    "blur_kernel_size = 111  # Adjust this value to change blur intensity\n",
    "opacity = 1  # 0.0 is fully transparent, 1.0 is fully opaque\n",
    "\n",
    "# Mouth landmarks for masking\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "#We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf))+ \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask+videoname, fourcc, \n",
    "                         fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                h, w, c = image.shape\n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    if masking:\n",
    "                        # Create mask for mouth area\n",
    "                        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get mouth area points\n",
    "                        mouth_points = np.array([(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) \n",
    "                                               for idx in MOUTH_LANDMARKS], dtype=np.int32)\n",
    "                        \n",
    "                        # Fill mouth polygon\n",
    "                        cv2.fillPoly(mask, [mouth_points], 255)\n",
    "                        \n",
    "                        # Apply Gaussian blur to the mouth area\n",
    "                        blur_region = cv2.GaussianBlur(original_image, (blur_kernel_size, blur_kernel_size), 0)\n",
    "                        \n",
    "                        # Blend original and blurred image based on mask and opacity\n",
    "                        mask_3channel = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (original_image * (1 - mask_3channel * opacity) + \n",
    "                                        blur_region * (mask_3channel * opacity)).astype(np.uint8)\n",
    "                    \n",
    "                    # Draw landmarks (optional - you might want to remove these for the final video)\n",
    "                    #mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #mp_drawing.draw_landmarks(\n",
    "                    #        original_image,\n",
    "                    #        results.face_landmarks,\n",
    "                    #        mp_holistic.FACEMESH_TESSELATION,\n",
    "                    #        landmark_drawing_spec=None,\n",
    "                    #        connection_drawing_spec=mp_drawing_styles\n",
    "                    #        .get_default_face_mesh_tesselation_style())\n",
    "                    #mp_drawing.draw_landmarks(\n",
    "                    #        original_image,\n",
    "                    #        results.pose_landmarks,\n",
    "                    #        mp_holistic.POSE_CONNECTIONS,\n",
    "                    #        landmark_drawing_spec=mp_drawing_styles.\n",
    "                    #        get_default_pose_landmarks_style())\n",
    "                    \n",
    "                    # Save time series data\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                if np.all(results.face_landmarks) == None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time+(1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Write CSV files\n",
    "    filebody = open(outtputf_ts + vidf[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "        \n",
    "    filehands = open(outtputf_ts + vidf[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "        \n",
    "    fileface = open(outtputf_ts + vidf[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f773cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "DOLFIJN.mp4\n",
      "This is video number 0 of 6 videos in total\n",
      "We will now process video:\n",
      "ETEN.mp4\n",
      "This is video number 1 of 6 videos in total\n",
      "We will now process video:\n",
      "NULL.mp4\n",
      "This is video number 2 of 6 videos in total\n",
      "We will now process video:\n",
      "OCHTEND.mp4\n",
      "This is video number 3 of 6 videos in total\n",
      "We will now process video:\n",
      "OLIFANT.mp4\n",
      "This is video number 4 of 6 videos in total\n",
      "We will now process video:\n",
      "RIETJE.mp4\n",
      "This is video number 5 of 6 videos in total\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# do you want to apply masking?\n",
    "masking = True\n",
    "blur_kernel_size = 111  # Adjust this value to change blur intensity\n",
    "opacity = 1  # 0.0 is fully transparent, 1.0 is fully opaque\n",
    "\n",
    "# Mouth landmarks for masking\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf))+ \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask+'handm_'+videoname, fourcc, \n",
    "                         fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                h, w, c = image.shape\n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    if masking:\n",
    "                        # Create mask for mouth area\n",
    "                        mouth_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get mouth area points\n",
    "                        mouth_points = np.array([(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) \n",
    "                                               for idx in MOUTH_LANDMARKS], dtype=np.int32)\n",
    "                        \n",
    "                        # Fill mouth polygon\n",
    "                        cv2.fillPoly(mouth_mask, [mouth_points], 255)\n",
    "                        \n",
    "                        # Create hand mask\n",
    "                        hand_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        \n",
    "                        # Draw hands on the mask\n",
    "                        if results.left_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.left_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                                \n",
    "                        if results.right_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.right_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                        \n",
    "                        # Dilate hand mask to create a more natural coverage\n",
    "                        kernel = np.ones((15,15), np.uint8)\n",
    "                        hand_mask = cv2.dilate(hand_mask, kernel, iterations=1)\n",
    "                        \n",
    "                        # Subtract hand area from mouth mask\n",
    "                        mouth_mask = cv2.subtract(mouth_mask, hand_mask)\n",
    "                        \n",
    "                        # Apply Gaussian blur to the mouth area\n",
    "                        blur_region = cv2.GaussianBlur(original_image, (blur_kernel_size, blur_kernel_size), 0)\n",
    "                        \n",
    "                        # Blend original and blurred image based on mask and opacity\n",
    "                        mask_3channel = cv2.cvtColor(mouth_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (original_image * (1 - mask_3channel * opacity) + \n",
    "                                        blur_region * (mask_3channel * opacity)).astype(np.uint8)\n",
    "                    \n",
    "                    # Save time series data\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                if np.all(results.face_landmarks) == None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time+(1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Write CSV files\n",
    "    filebody = open(outtputf_ts + vidf[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "        \n",
    "    filehands = open(outtputf_ts + vidf[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "        \n",
    "    fileface = open(outtputf_ts + vidf[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5e7270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "DOLFIJN.mp4\n",
      "This is video number 0 of 6 videos in total\n",
      "We will now process video:\n",
      "ETEN.mp4\n",
      "This is video number 1 of 6 videos in total\n",
      "We will now process video:\n",
      "NULL.mp4\n",
      "This is video number 2 of 6 videos in total\n",
      "We will now process video:\n",
      "OCHTEND.mp4\n",
      "This is video number 3 of 6 videos in total\n",
      "We will now process video:\n",
      "OLIFANT.mp4\n",
      "This is video number 4 of 6 videos in total\n",
      "We will now process video:\n",
      "RIETJE.mp4\n",
      "This is video number 5 of 6 videos in total\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# do you want to apply masking?\n",
    "masking = True\n",
    "blur_kernel_size = 111  # Adjust this value to change blur intensity\n",
    "opacity = 1  # 0.0 is fully transparent, 1.0 is fully opaque\n",
    "\n",
    "# Mouth landmarks for masking\n",
    "MOUTH_LANDMARKS = [192, 206, 2, 426, 436, 434, 431, 211]\n",
    "\n",
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf))+ \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask+'handv3_'+videoname, fourcc, \n",
    "                         fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                \n",
    "                h, w, c = image.shape\n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    if masking:\n",
    "                        # Create mask for mouth area\n",
    "                        mouth_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get mouth area points\n",
    "                        mouth_points = np.array([(int(landmarks[idx].x * w), int(landmarks[idx].y * h)) \n",
    "                                               for idx in MOUTH_LANDMARKS], dtype=np.int32)\n",
    "                        \n",
    "                        # Fill mouth polygon\n",
    "                        cv2.fillPoly(mouth_mask, [mouth_points], 255)\n",
    "                        \n",
    "                        # Create hand mask\n",
    "                        hand_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        \n",
    "                        # Draw hands on the mask\n",
    "                        if results.left_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.left_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                                \n",
    "                        if results.right_hand_landmarks:\n",
    "                            hand_points = []\n",
    "                            for landmark in results.right_hand_landmarks.landmark:\n",
    "                                x = int(landmark.x * w)\n",
    "                                y = int(landmark.y * h)\n",
    "                                hand_points.append((x, y))\n",
    "                            if len(hand_points) > 0:\n",
    "                                hull = cv2.convexHull(np.array(hand_points))\n",
    "                                cv2.fillConvexPoly(hand_mask, hull, 255)\n",
    "                        \n",
    "                        # Create a more precise hand mask with minimal dilation\n",
    "                        kernel = np.ones((5,5), np.uint8)\n",
    "                        hand_mask = cv2.dilate(hand_mask, kernel, iterations=1)\n",
    "                        \n",
    "                        # Smooth the edges of the hand mask\n",
    "                        hand_mask = cv2.GaussianBlur(hand_mask, (3,3), 0)\n",
    "                        _, hand_mask = cv2.threshold(hand_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "                        \n",
    "                        # First apply the mouth blur\n",
    "                        blur_region = cv2.GaussianBlur(original_image, (blur_kernel_size, blur_kernel_size), 0)\n",
    "                        mask_3channel = cv2.cvtColor(mouth_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        blurred_image = (original_image * (1 - mask_3channel * opacity) + \n",
    "                                       blur_region * (mask_3channel * opacity)).astype(np.uint8)\n",
    "                        \n",
    "                        # Then overlay the hands\n",
    "                        hand_mask_3channel = cv2.cvtColor(hand_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (blurred_image * (1 - hand_mask_3channel) + \n",
    "                                        original_image * hand_mask_3channel).astype(np.uint8)\n",
    "                    \n",
    "                    # Save time series data\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                if np.all(results.face_landmarks) == None:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time+(1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Write CSV files\n",
    "    filebody = open(outtputf_ts + vidf[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "        \n",
    "    filehands = open(outtputf_ts + vidf[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "        \n",
    "    fileface = open(outtputf_ts + vidf[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
