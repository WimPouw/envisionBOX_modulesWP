{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93545df",
   "metadata": {},
   "source": [
    "# Using Media-Pipe for full-body tracking, masking, blurring, and movement tracing\n",
    "<br>\n",
    "<div align=\"center\">Wim Pouw (wim.pouw@donders.ru.nl) & Sho Akamine (Sho.Akamine@mpi.nl)</div>\n",
    "\n",
    "<img src=\"Images/envision_banner.png\" alt=\"isolated\" width=\"300\"/>\n",
    "<img src=\"Images/options.gif\" alt=\"isolated\" width=\"600\"/>\n",
    "\n",
    "## Info documents\n",
    "\n",
    "This python notebook runs you through the procedure of taking videos as inputs with a single person in the video, and outputting outputs of the kinematic timeseries, and optionally masking, blurring, and adding hand movement traces to videos with facial, hand, and arm skeletons.\n",
    "\n",
    "The current code is derived and modified from the masked-piper tool, which is a simple but effective modification of the the Holistic Tracking by Google's Mediapipe so that we can use it as a CPU-based light weigth tool to mask your video data while maintaining background information, and also preserving information about body kinematics. \n",
    "\n",
    "* location Repository:  https://github.com/WimPouw/envisionBOX_modulesWP/tree/main/Mediapipe_Optional_Masking\n",
    "\n",
    "* location Jupyter notebook: https://github.com/WimPouw/envisionBOX_modulesWP/blob/main/MultimodalMerging/Masking_Mediapiping.ipynb\n",
    "\n",
    "Current Github: https://github.com/WimPouw/TowardsMultimodalOpenScience\n",
    "\n",
    "## Version\n",
    "2.0.0 (we added some functionalities such as blurring and movement tracing, and fixed some bugs with non-present right/left body parts)\n",
    "\n",
    "## Additional information backbone of the tool (Mediapipe Holistic Tracking)\n",
    "https://google.github.io/mediapipe/solutions/holistic.html\n",
    "\n",
    "## Citation of mediapipe\n",
    "citation: Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., ... & Grundmann, M. (2019). Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172.\n",
    "\n",
    "## Citation of masked piper\n",
    "* citation: Owoyele, B., Trujillo, J., De Melo, G., & Pouw, W. (2022). Masked-Piper: Masking personal identities in visual recordings while preserving multimodal information. SoftwareX, 20, 101236. \n",
    "* Original Repo: https://github.com/WimPouw/TowardsMultimodalOpenScience\n",
    "\n",
    "## Citation of this code\n",
    "* citation: Pouw, W., & Akamine, S. (2025). Using Media-Pipe for full-body tracking, masking, blurring, and movement tracing. Retrieved from https://github.com/WimPouw/envisionBOX_modulesWP/tree/main/Mediapipe_Optional_Masking\n",
    "\n",
    "## Use\n",
    "Make sure to install all the packages in requirements.txt. Then move your videos that you want to mask into the input folder. Then run this code, which will loop through all the videos contained in the input folder; and saves all the results in the output folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15752c8c",
   "metadata": {},
   "source": [
    "## Setting up packages and folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f423ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder is set as the output folder where all the pose time series are stored\n",
      "d:\\Research_projects\\envisionBOX_modulesWP\\Mediapipe_Optional_Masking\\Output_TimeSeries\n",
      "\n",
      " The following folder is set as the output folder for saving the masked videos \n",
      "d:\\Research_projects\\envisionBOX_modulesWP\\Mediapipe_Optional_Masking\\Output_Videos\n",
      "\n",
      " The following video(s) will be processed for masking: \n",
      "['ted_kid.mp4']\n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./Input_Videos/\" #this is your folder with (all) your video(s)\n",
    "vfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))] #loop through the filenames and collect them in a list\n",
    "#time series output folder\n",
    "inputfol = \"./Input_Videos/\"\n",
    "outputf_mask = \"./Output_Videos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outtputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_mask))\n",
    "print(\"\\n The following video(s) will be processed for masking: \")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30a174",
   "metadata": {},
   "source": [
    "## Initializing marker names and key methods from mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1815d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06712ae9",
   "metadata": {},
   "source": [
    "## Main procedure Masked-Piper\n",
    "The following chunk of code loops through all the videos you have loaded into the input folder, then assess each frame for body poses, extract kinematic info, masks, blurs the body or face in a new frame that keeps (or removes) the background, projects the kinematic info on the mask, and stores the kinematic info for that frame into the time series .csv for the hand + body + face.\n",
    "\n",
    "We advise you to play around with the settings to see what animations you like best. The default setting now is full body blur with full body skeleton imposed on the image. We also enable tracing of the index fingers for motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ted_kid.mp4\n",
      "Video 1 of 1\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "# MASKING AND BLURRING OPTIONS \n",
    "skeleton = True\n",
    "skeleton_face_only = False         # Only show face skeleton (no body, no hands)\n",
    "whitebackground = False            # Grey background with skeleton only (no body, no face)\n",
    "maskingbody = False                # Masks the body silhouette with fully black color (original masked-piper approach)\n",
    "maskingface = False                # Masks the face (fully black color)\n",
    "blurringface = False               # Blurs the face\n",
    "blurringbody = True                # Blurs the body region\n",
    "blurringfactor = 1                 # Blurring intensity (0-1, 1 is full blur)\n",
    "# TRACE OPTIONS\n",
    "add_finger_traces = True           # Add fading traces for index fingers\n",
    "trace_length_seconds = 1.5         # Length of trace in seconds\n",
    "trace_color_left = (0, 255, 0)     # Green for left index finger trace\n",
    "trace_color_right = (0, 0, 255)    # Blue for right index finger trace\n",
    "\n",
    "# Process videos\n",
    "# MASKING AND BLURRING OPTIONS \n",
    "skeleton = True\n",
    "skeleton_face_only = False         # Only show face skeleton (no body, no hands)\n",
    "whitebackground = False            # Grey background with skeleton only (no body, no face)\n",
    "maskingbody = False                # Masks the body silhouette with fully black color (original masked-piper approach)\n",
    "maskingface = False                # Masks the face (fully black color)\n",
    "blurringface = False               # Blurs the face\n",
    "blurringbody = True                # Blurs the body region\n",
    "blurringfactor = 1                 # Blurring intensity (0-1, 1 is full blur)\n",
    "# TRACE OPTIONS\n",
    "add_finger_traces = True           # Add fading traces for index fingers\n",
    "trace_length_seconds = 2           # Length of trace in seconds\n",
    "trace_color_left = (0, 255, 0)     # Green for left index finger trace\n",
    "trace_color_right = (0, 0, 255)    # Blue for right index finger trace\n",
    "\n",
    "# Process videos\n",
    "for vidf in vfiles:\n",
    "    print(f\"Processing video: {vidf}\")\n",
    "    print(f\"Video {vfiles.index(vidf)+1} of {len(vfiles)}\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask + videoname, fourcc, \n",
    "                          fps=samplerate, frameSize=(int(frameWidth), int(frameHeight)))\n",
    "    \n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    # Initialize trace buffers for index fingers\n",
    "    if add_finger_traces:\n",
    "        trace_frames = int(trace_length_seconds * samplerate)\n",
    "        left_finger_trace = []  # Store (x, y) positions for left index finger\n",
    "        right_finger_trace = []  # Store (x, y) positions for right index finger\n",
    "    \n",
    "    with mp_holistic.Holistic(static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while True:\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                h, w, c = image.shape\n",
    "                \n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    # Apply white background modes first (they override other options)\n",
    "                    if whitebackground:\n",
    "                        # Create white background with only landmarks\n",
    "                        white_image = np.full((h, w, 3), (255, 255, 255), dtype=np.uint8)\n",
    "                        original_image = cv2.cvtColor(white_image, cv2.COLOR_RGB2BGR)\n",
    "                    else:\n",
    "                        # Convert to BGR for further processing\n",
    "                        original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Apply body masking if enabled (makes body fully black - original masked-piper)\n",
    "                    if maskingbody and not whitebackground:\n",
    "                        # Original masking logic\n",
    "                        image_with_alpha = np.concatenate([image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        mask_img = np.zeros_like(image, dtype=np.uint8)\n",
    "                        mask_img[:, :] = (255,255,255)\n",
    "                        segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "                        segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
    "                        annotated_image = mask_img * segm_2class * (1 - segm_2class)\n",
    "                        mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        image_with_alpha[mask==0]=0\n",
    "                        original_image = cv2.cvtColor(image_with_alpha, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Apply blurring to body if enabled\n",
    "                    if blurringbody and not whitebackground:\n",
    "                        # Get body mask\n",
    "                        segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "                        body_mask = segm_2class\n",
    "                        \n",
    "                        kernel_size = int(51 * blurringfactor)\n",
    "                        if kernel_size % 2 == 0:\n",
    "                            kernel_size += 1\n",
    "                        blurred_image = cv2.GaussianBlur(original_image, (kernel_size, kernel_size), 0)\n",
    "                        \n",
    "                        # Apply blur only to body region\n",
    "                        body_mask_3channel = cv2.merge([body_mask] * 3)\n",
    "                        original_image = (original_image * (1 - body_mask_3channel * blurringfactor) + \n",
    "                                        blurred_image * (body_mask_3channel * blurringfactor)).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply face blurring if enabled\n",
    "                    if blurringface and results.face_landmarks and not whitebackground:\n",
    "                        face_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get all face points for a full face mask\n",
    "                        face_points = np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) \n",
    "                                              for i in range(len(landmarks))], dtype=np.int32)\n",
    "                        \n",
    "                        # Create convex hull of face points\n",
    "                        hull = cv2.convexHull(face_points)\n",
    "                        cv2.fillConvexPoly(face_mask, hull, 255)\n",
    "                        \n",
    "                        # Blur the face region\n",
    "                        kernel_size = int(51 * blurringfactor)\n",
    "                        if kernel_size % 2 == 0:\n",
    "                            kernel_size += 1\n",
    "                        blurred_image = cv2.GaussianBlur(original_image, (kernel_size, kernel_size), 0)\n",
    "                        \n",
    "                        # Apply blur only to face region\n",
    "                        face_mask_3channel = cv2.cvtColor(face_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (original_image * (1 - face_mask_3channel * blurringfactor) + \n",
    "                                        blurred_image * (face_mask_3channel * blurringfactor)).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply face masking if enabled (makes face fully black)\n",
    "                    if maskingface and results.face_landmarks and not whitebackground:\n",
    "                        face_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Create mask for entire face\n",
    "                        face_points = np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) \n",
    "                                              for i in range(len(landmarks))], dtype=np.int32)\n",
    "                        \n",
    "                        # Create convex hull of face points for masking\n",
    "                        hull = cv2.convexHull(face_points)\n",
    "                        cv2.fillConvexPoly(face_mask, hull, 255)\n",
    "                        \n",
    "                        # Apply masking - make face fully black\n",
    "                        original_image[face_mask > 0] = (0, 0, 0)  # Make face fully black\n",
    "                    \n",
    "                    # Add index finger traces if enabled\n",
    "                    if add_finger_traces:\n",
    "                        # Get current index finger positions\n",
    "                        left_finger_pos = None\n",
    "                        right_finger_pos = None\n",
    "                        \n",
    "                        if results.left_hand_landmarks:\n",
    "                            # Index finger tip landmark index is 8\n",
    "                            left_landmark = results.left_hand_landmarks.landmark[8]\n",
    "                            left_finger_pos = (int(left_landmark.x * w), int(left_landmark.y * h))\n",
    "                        \n",
    "                        if results.right_hand_landmarks:\n",
    "                            # Index finger tip landmark index is 8\n",
    "                            right_landmark = results.right_hand_landmarks.landmark[8]\n",
    "                            right_finger_pos = (int(right_landmark.x * w), int(right_landmark.y * h))\n",
    "                        \n",
    "                        # Add current positions to trace buffers\n",
    "                        if left_finger_pos:\n",
    "                            left_finger_trace.append(left_finger_pos)\n",
    "                        if right_finger_pos:\n",
    "                            right_finger_trace.append(right_finger_pos)\n",
    "                        \n",
    "                        # Keep trace buffers to specified length\n",
    "                        if len(left_finger_trace) > trace_frames:\n",
    "                            left_finger_trace.pop(0)\n",
    "                        if len(right_finger_trace) > trace_frames:\n",
    "                            right_finger_trace.pop(0)\n",
    "                        \n",
    "                        # Draw fading traces for left index finger\n",
    "                        for i in range(len(left_finger_trace)-1):\n",
    "                            if i < len(left_finger_trace) and (i+1) < len(left_finger_trace):\n",
    "                                # Calculate opacity based on position in trace\n",
    "                                opacity = int(255 * (i+1) / len(left_finger_trace))\n",
    "                                alpha = opacity / 255.0\n",
    "                                \n",
    "                                # Create a copy for transparency effect\n",
    "                                overlay = original_image.copy()\n",
    "                                cv2.line(overlay, left_finger_trace[i], left_finger_trace[i+1], \n",
    "                                       trace_color_left, 2)\n",
    "                                original_image = cv2.addWeighted(overlay, alpha, original_image, 1-alpha, 0)\n",
    "                        \n",
    "                        # Draw fading traces for right index finger\n",
    "                        for i in range(len(right_finger_trace)-1):\n",
    "                            if i < len(right_finger_trace) and (i+1) < len(right_finger_trace):\n",
    "                                # Calculate opacity based on position in trace\n",
    "                                opacity = int(255 * (i+1) / len(right_finger_trace))\n",
    "                                alpha = opacity / 255.0\n",
    "                                \n",
    "                                # Create a copy for transparency effect\n",
    "                                overlay = original_image.copy()\n",
    "                                cv2.line(overlay, right_finger_trace[i], right_finger_trace[i+1], \n",
    "                                       trace_color_right, 2)\n",
    "                                original_image = cv2.addWeighted(overlay, alpha, original_image, 1-alpha, 0)\n",
    "                    \n",
    "                    # Draw landmarks conditionally\n",
    "                    if skeleton:\n",
    "                        mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        \n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                        )\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.pose_landmarks,\n",
    "                            mp_holistic.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "                        )\n",
    "                    elif skeleton_face_only:\n",
    "                        # For face-only mode, draw only face landmarks\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                        )\n",
    "                    \n",
    "                    # Save time series data - FIXED for hand swapping issue\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    \n",
    "                    # Process hands separately\n",
    "                    sampleLH = listpostions(results.left_hand_landmarks)\n",
    "                    sampleRH = listpostions(results.right_hand_landmarks)\n",
    "                    \n",
    "                    # Fill empty left hand with placeholders\n",
    "                    if len(sampleLH) == 0:\n",
    "                        sampleLH = [\"\" for x in range(int(len(markerxyzhands)/2))]\n",
    "                    \n",
    "                    # Combine hands\n",
    "                    samplehands = sampleLH + sampleRH\n",
    "                    \n",
    "                    # Add time\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    \n",
    "                    # Append to time series\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                else:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    # Add NaN data\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time + (1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "    \n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Save CSV files\n",
    "    filebody = open(outtputf_ts + vidf[:-4] + '_body.csv', 'w+', newline='')\n",
    "    with filebody:\n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "    \n",
    "    filehands = open(outtputf_ts + vidf[:-4] + '_hands.csv', 'w+', newline='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "    \n",
    "    fileface = open(outtputf_ts + vidf[:-4] + '_face.csv', 'w+', newline='')\n",
    "    with fileface:\n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca47f5",
   "metadata": {},
   "source": [
    "## Example output (full body blur + skeleton + Tracing)\n",
    "<img src=\"Images/ted_kid.gif\" alt=\"isolated\" width=\"600\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
